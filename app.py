import gradio as gr
import whisper
import os
import tempfile
import base64
import json
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

# Load Whisper model (small model - balance giá»¯a tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c)
print("Äang load Whisper model...")
model = whisper.load_model("small")
print("âœ… ÄÃ£ load Whisper model thÃ nh cÃ´ng!")

def transcribe_audio(audio_file):
    """
    Transcribe audio file báº±ng Whisper model

    Args:
        audio_file: Audio file tá»« Gradio (cÃ³ thá»ƒ lÃ  file path hoáº·c tuple)

    Returns:
        str: Text Ä‘Ã£ transcribe
    """
    try:
        # Xá»­ lÃ½ input tá»« Gradio
        if audio_file is None:
            return "âŒ Vui lÃ²ng upload file audio!"

        # Náº¿u audio_file lÃ  tuple (tá»« microphone), láº¥y file path
        if isinstance(audio_file, tuple):
            audio_path = audio_file[0]
        else:
            audio_path = audio_file

        print(f"ğŸ¤ Äang transcribe: {audio_path}")

        # Transcribe audio
        result = model.transcribe(
            audio_path,
            language="en",  # Auto-detect language, cÃ³ thá»ƒ Ä‘á»•i thÃ nh "vi" cho tiáº¿ng Viá»‡t
            fp16=False,     # Táº¯t FP16 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i CPU
            verbose=False
        )

        transcribed_text = result["text"].strip()
        print(f"âœ… Káº¿t quáº£: {transcribed_text}")

        return transcribed_text

    except Exception as e:
        error_msg = f"âŒ Lá»—i khi transcribe: {str(e)}"
        print(error_msg)
        return error_msg

def transcribe_base64(base64_audio, language="en"):
    """
    Transcribe audio tá»« base64 string

    Args:
        base64_audio: Audio Ä‘Ã£ encode base64
        language: NgÃ´n ngá»¯ (máº·c Ä‘á»‹nh "en", dÃ¹ng None Ä‘á»ƒ auto-detect)

    Returns:
        dict: {"text": transcribed_text, "success": True/False}
    """
    try:
        # Decode base64 thÃ nh bytes
        audio_bytes = base64.b64decode(base64_audio)

        # Táº¡o file táº¡m Ä‘á»ƒ Whisper xá»­ lÃ½
        with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as temp_audio:
            temp_audio.write(audio_bytes)
            temp_audio_path = temp_audio.name

        print(f"ğŸ¤ Äang transcribe tá»« base64 (size: {len(audio_bytes)} bytes)")

        # Transcribe audio
        result = model.transcribe(
            temp_audio_path,
            language=language if language != "auto" else None,
            fp16=False,
            verbose=False
        )

        transcribed_text = result["text"].strip()

        # XÃ³a file táº¡m
        try:
            os.unlink(temp_audio_path)
        except:
            pass

        print(f"âœ… Káº¿t quáº£: {transcribed_text}")

        return {
            "success": True,
            "text": transcribed_text,
            "language": result.get("language", "unknown")
        }

    except Exception as e:
        error_msg = f"Lá»—i khi transcribe: {str(e)}"
        print(f"âŒ {error_msg}")
        return {
            "success": False,
            "error": error_msg
        }

# Táº¡o Gradio Interface
with gr.Blocks(title="Whisper Audio Transcription") as demo:
    gr.Markdown("""
    # ğŸ¤ Whisper Audio Transcription

    Upload file audio hoáº·c record trá»±c tiáº¿p Ä‘á»ƒ chuyá»ƒn Ä‘á»•i giá»ng nÃ³i thÃ nh text báº±ng OpenAI Whisper.

    **Model:** Whisper Small (chÃ­nh xÃ¡c cao, tá»‘c Ä‘á»™ vá»«a pháº£i)

    **Há»— trá»£:** MP3, WAV, M4A, vÃ  nhiá»u Ä‘á»‹nh dáº¡ng audio khÃ¡c
    """)

    with gr.Tab("ğŸ“ Upload File"):
        audio_input_file = gr.Audio(
            label="Upload Audio File",
            type="filepath",
            sources=["upload"]
        )
        transcribe_btn_file = gr.Button("ğŸš€ Transcribe", variant="primary")
        output_text_file = gr.Textbox(
            label="Káº¿t quáº£ Transcription",
            placeholder="Text sáº½ hiá»‡n á»Ÿ Ä‘Ã¢y...",
            lines=10
        )

        transcribe_btn_file.click(
            fn=transcribe_audio,
            inputs=audio_input_file,
            outputs=output_text_file
        )

    with gr.Tab("ğŸ™ï¸ Record Audio"):
        audio_input_mic = gr.Audio(
            label="Record Audio",
            type="filepath",
            sources=["microphone"]
        )
        transcribe_btn_mic = gr.Button("ğŸš€ Transcribe", variant="primary")
        output_text_mic = gr.Textbox(
            label="Káº¿t quáº£ Transcription",
            placeholder="Text sáº½ hiá»‡n á»Ÿ Ä‘Ã¢y...",
            lines=10
        )

        transcribe_btn_mic.click(
            fn=transcribe_audio,
            inputs=audio_input_mic,
            outputs=output_text_mic
        )

    with gr.Tab("ğŸ” Base64 Input (API)"):
        gr.Markdown("""
        ### Gá»­i audio dÆ°á»›i dáº¡ng Base64
        Nháº­p chuá»—i base64 cá»§a audio file Ä‘á»ƒ transcribe (giá»‘ng nhÆ° trong code cá»§a báº¡n)
        """)

        base64_input = gr.Textbox(
            label="Base64 Audio String",
            placeholder="Paste base64 string cá»§a audio vÃ o Ä‘Ã¢y...",
            lines=5
        )

        language_select = gr.Dropdown(
            choices=["auto", "en", "vi", "zh", "ja", "ko", "es", "fr", "de"],
            value="auto",
            label="NgÃ´n ngá»¯ (auto = tá»± Ä‘á»™ng detect)"
        )

        transcribe_btn_base64 = gr.Button("ğŸš€ Transcribe Base64", variant="primary")

        output_json = gr.JSON(label="Káº¿t quáº£ JSON")

        transcribe_btn_base64.click(
            fn=transcribe_base64,
            inputs=[base64_input, language_select],
            outputs=output_json
        )

    gr.Markdown("""
    ---
    ### ğŸ“ LÆ°u Ã½:
    - Model há»— trá»£ nhiá»u ngÃ´n ngá»¯ (auto-detect)
    - Cháº¥t lÆ°á»£ng audio tá»‘t hÆ¡n = káº¿t quáº£ chÃ­nh xÃ¡c hÆ¡n
    - Thá»i gian xá»­ lÃ½ phá»¥ thuá»™c vÃ o Ä‘á»™ dÃ i audio
    - **Tab Base64**: DÃ¹ng Ä‘á»ƒ test API vá»›i base64 input (nhÆ° trong code cá»§a báº¡n)

    ### ğŸ”§ Powered by:
    - OpenAI Whisper (Small model)
    - Gradio
    - Hugging Face Spaces
    """)

# Launch app
if __name__ == "__main__":
    demo.launch()
